{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "try:\n",
    "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    print(\"Ollama reachable\")\n",
    "    print(\"Available models:\", [m[\"name\"] for m in r.json().get(\"models\", [])])\n",
    "except Exception as e:\n",
    "    print(\"Ollama NOT reachable \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "input_prompts = [\n",
    "    \"Smile\", \"Joy\", \"Sadness\", \"Anger\", \"Fear\", \"Love\", \"Hope\", \"Peace\", \"Calm\", \"Rage\", \"Happy\", \"Excited\", \"Nervous\", \"Confident\", \"Worried\", \"Grateful\", \"Lonely\", \"Pride\",\n",
    "    \"Apple\", \"Car\", \"House\", \"Tree\", \"Ocean\", \"Mountain\", \"Book\", \"Phone\", \"Computer\", \"Chair\",  \"Table\", \"Window\", \"Door\", \"Key\", \"Lamp\", \"Mirror\", \"Painting\", \"Clock\", \"Flower\", \"Stone\",\n",
    "    \"Run\", \"Jump\", \"Dance\", \"Sleep\", \"Think\", \"Write\", \"Read\", \"Listen\", \"Watch\", \"Create\",  \"Build\", \"Destroy\", \"Help\", \"Learn\", \"Teach\", \"Play\", \"Work\", \"Rest\", \"Travel\", \"Explore\",\n",
    "    \"Freedom\", \"Justice\", \"Truth\", \"Beauty\", \"Wisdom\", \"Courage\", \"Honor\", \"Faith\", \"Trust\", \"Mystery\",  \"Future\", \"Past\", \"Present\", \"Infinity\", \"Nothing\", \"Everything\", \"Reality\", \"Dream\", \"Memory\", \"Imagination\",\n",
    "    \"Red\", \"Blue\", \"Green\", \"Yellow\", \"Purple\", \"Orange\", \"Black\", \"White\", \"Pink\", \"Brown\", \"Cat\", \"Dog\", \"Bird\", \"Fish\", \"Lion\", \"Tiger\", \"Elephant\", \"Horse\", \"Rabbit\", \"Snake\", \"Pizza\", \"Cake\", \"Bread\", \"Milk\", \"Coffee\", \"Tea\", \"Rice\", \"Pasta\", \"Soup\", \"Salad\",\n",
    "    \"Algorithm\", \"Database\", \"Network\", \"Security\", \"Interface\", \"Protocol\", \"Framework\", \"Library\",  \"Function\", \"Variable\", \"Loop\", \"Array\", \"Object\", \"Class\", \"Method\", \"API\",\n",
    "    \"A\", \"B\", \"C\", \"X\", \"Y\", \"Z\",  \"One\", \"Two\", \"Five\", \"Ten\", \"Hundred\", \"Thousand\", \"Million\", \"Zero\", \"\", \" \", \".\", \"?\", \"!\", \"...\", \"???\", \"The\", \"And\", \"Or\", \"But\", \"If\", \"When\", \"Why\", \"How\", \"What\",\n",
    "    \"Quantum\", \"Gravity\", \"Energy\", \"Matter\", \"Space\", \"Time\", \"Evolution\", \"DNA\", \"Atom\", \"Universe\",\"Existence\", \"Consciousness\", \"Soul\", \"Mind\", \"Body\", \"Spirit\", \"Ethics\", \"Morality\", \"Purpose\", \"Meaning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inference(input_prompt, output_text, file_number):\n",
    "    \"\"\"Save inference to a specific file\"\"\"\n",
    "    filename = f\"inference/inference_data_{file_number:04d}.txt\"\n",
    "    \n",
    "    with open(filename, \"a\", encoding=\"utf-8\") as file:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        file.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        file.write(f\"Input: {input_prompt}\\n\")\n",
    "        file.write(f\"Output: {output_text}\\n\")\n",
    "        file.write(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "This make_inference function sends a text prompt to a local Ollama AI model (gpt-oss:20b) via HTTP POST request and returns the generated response text.\n",
    "It includes retry logic with exponential backoff to handle connection failures, attempting up to 3 times before giving up and returning an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(selected_prompt):\n",
    "    \"\"\"Send one prompt to the model and return text or error string.\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": \"gpt-oss:20b\",\n",
    "        \"prompt\": selected_prompt,\n",
    "        \"stream\": False,\n",
    "        \"raw\": True,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_predict\": 4000,\n",
    "            \"num_ctx\": 2100,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    }\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, json=data, timeout=180)\n",
    "            result = response.json()\n",
    "            return result.get(\"response\", \"ERROR: No response\")\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = (2 ** attempt) * 5  \n",
    "                print(f\"Attempt {attempt + 1} failed, retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return f\"ERROR after {max_retries} attempts: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_inferences = 1_000_000\n",
    "inferences_per_file = 1000 \n",
    "os.makedirs(\"inference\", exist_ok=True)\n",
    "print(f\"Starting {total_inferences:,} inferences...\")\n",
    "print(f\"Saving {inferences_per_file} inferences per file\")\n",
    "print(f\"Total prompts available: {len(input_prompts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "This code below runs a loop that makes 1 million AI inferences by randomly selecting prompts from a predefined list, sending each to the GPT model, and saving both the input prompt and output response to organized text files.\n",
    "It processes the inferences with a 5-second delay between each call, saves 1000 inferences per file, and provides progress updates every 100 completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(total_inferences):\n",
    "    selected_prompt = random.choice(input_prompts)\n",
    "    file_number = (i // inferences_per_file) + 1\n",
    "\n",
    "    print(f\"Inference {i+1:,}/{total_inferences:,} - File {file_number} - Prompt: {repr(selected_prompt)}\")\n",
    "    output = make_inference(selected_prompt)\n",
    "\n",
    "    save_inference(selected_prompt, output, file_number)\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Completed {i+1:,} inferences ({((i+1)/total_inferences)*100:.2f}%)\")\n",
    "\n",
    "    time.sleep(5.0)\n",
    "\n",
    "print(f\"Completed all {total_inferences:,} inferences!\")\n",
    "print(f\"Data saved across {(total_inferences // inferences_per_file)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Till the previous sell, we made 1 million infrences using gpt oss:20b, using it without the harmony formate. \n",
    "After this sell, we will create the embeddins of input query, and the infrences a model made for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "infrence_file = \"infrences.txt\"\n",
    "\n",
    "os.makedirs(\"embeddings\", exist_ok=True)\n",
    "input_emb_file   = \"embeddings/input_embeddings.txt\"  \n",
    "chunks_text_file = \"embeddings/output_chunks.txt\" \n",
    "output_emb_file  = \"embeddings/output_embeddings.txt\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    resp = openai.embeddings.create(input=text, model=model)\n",
    "    return np.array(resp.data[0].embedding)\n",
    "\n",
    "def chunk_text(long_sentence, chunk_size=400, overlap=100):\n",
    "    \"\"\"Split by words into overlapping chunks (same pattern you used).\"\"\"\n",
    "    words = long_sentence.split()\n",
    "    chunks, start = [], 0\n",
    "    step = max(chunk_size - overlap, 1)\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk_text = \" \".join(words[start:end])\n",
    "        chunks.append(chunk_text)\n",
    "        if end >= len(words):\n",
    "            break\n",
    "        start += step\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(infrence_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# split the file into blocks starting with 'Timestamp:'\n",
    "blocks = re.split(r'(?=^Timestamp:\\s)', content, flags=re.MULTILINE)\n",
    "blocks = [b.strip() for b in blocks if b.strip()]\n",
    "\n",
    "records = []\n",
    "for b in blocks:\n",
    "    m_in  = re.search(r'^Input:\\s*(.*)$', b, flags=re.MULTILINE)\n",
    "    m_out = re.search(r'^Output:\\s*(.*)$', b, flags=re.MULTILINE | re.DOTALL)\n",
    "    if not (m_in and m_out):\n",
    "        continue\n",
    "\n",
    "    input_text = m_in.group(1).strip()\n",
    "\n",
    "    # everything after the 'Output:' line is output; include tail on same line\n",
    "    out_tail = m_out.group(1).strip()\n",
    "    out_body = b[m_out.end():].strip()\n",
    "    output_text = (out_tail + (\"\\n\" + out_body if out_body else \"\")).strip()\n",
    "\n",
    "    records.append({\"input\": input_text, \"output\": output_text})\n",
    "    if len(records) == 10:\n",
    "        break\n",
    "# Skip the first 3 records and keep only records 4-13\n",
    "records = records[:50]  \n",
    "\n",
    "#print(f\"Parsed {len(records)} inference(s).\")\n",
    "print(f\"Parsed {len(records)} inference(s) (inferences 1-10).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "embed inputs, chunk outputs, save chunks text, embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rec in enumerate(records, 1):\n",
    "    query = rec[\"input\"]\n",
    "    output_joined = \" \".join(rec[\"output\"].split())  # flatten whitespace\n",
    "\n",
    "    # 1) input embedding\n",
    "    q_emb = get_openai_embedding(query)\n",
    "    with open(input_emb_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Inference {i}\\n\")\n",
    "        f.write(f\"Query: {query}\\n\")\n",
    "        f.write(f\"Embedding: {q_emb.tolist()}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    # 2) output → chunks (sentences)\n",
    "    chunks = chunk_text(output_joined, chunk_size=400, overlap=100)\n",
    "\n",
    "    # save the sentences \n",
    "    with open(chunks_text_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== Inference {i} ===\\n\")\n",
    "        f.write(f\"Query: {query}\\n\")\n",
    "        for j, ch in enumerate(chunks, 1):\n",
    "            f.write(f\"Sentence {j}: {ch}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    # 3) embeddings for each sentence\n",
    "    with open(output_emb_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        for j, ch in enumerate(chunks, 1):\n",
    "            emb = get_openai_embedding(ch)\n",
    "            f.write(f\"Inference {i} | Sentence {j}: {emb.tolist()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Now, we have created embeddings, so we will moves towards computing the cosine distance between \n",
    "the query embedding and the sentences embedding of the corresponding infrence (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, ast, numpy as np\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "\n",
    "output_dir = \"embeddings\"\n",
    "input_emb_file   = os.path.join(output_dir, \"input_embeddings.txt\")\n",
    "output_emb_file  = os.path.join(output_dir, \"output_embeddings.txt\")\n",
    "cos_distance     = os.path.join(output_dir, \"inference1_cosine_distances.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "This below function searches through an input embeddings file to find and extract the numerical embedding vector for a specific inference number.\n",
    "\n",
    "It parses the file line by line, locates the section for the requested inference, finds the \"Embedding:\" line, and converts the string representation back into a NumPy array of floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_embedding(inference_index=1, path=input_emb_file) -> np.ndarray:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if lines[i].strip() == f\"Inference {inference_index}\":\n",
    "            j = i + 1\n",
    "            while j < len(lines) and not lines[j].startswith(\"Inference \"):\n",
    "                if lines[j].startswith(\"Embedding:\"):\n",
    "                    arr_str = lines[j].split(\"Embedding:\", 1)[1].strip()\n",
    "                    return np.array(ast.literal_eval(arr_str), dtype=np.float32)\n",
    "                j += 1\n",
    "        i += 1\n",
    "    raise ValueError(f\"Query embedding for inference {inference_index} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "This below function extracts all sentence embeddings for a specific inference from an output embeddings file using regex pattern matching to find lines with the format \"Inference X | Sentence Y: [embedding_vector]\".\n",
    "\n",
    "It collects all the sentence embeddings into a dictionary, sorts them by sentence number, and returns them as a list of NumPy arrays in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_embeddings(inference_index=1, path=output_emb_file) -> list[np.ndarray]:\n",
    "    sent_map = {}\n",
    "    pat = re.compile(rf\"^Inference {inference_index} \\| Sentence (\\d+):\\s*(\\[.*\\])$\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            m = pat.match(line.strip())\n",
    "            if m:\n",
    "                sidx = int(m.group(1))\n",
    "                vec = np.array(ast.literal_eval(m.group(2)), dtype=np.float32)\n",
    "                sent_map[sidx] = vec\n",
    "    if not sent_map:\n",
    "        raise ValueError(f\"No sentence embeddings for inference {inference_index}.\")\n",
    "    return [sent_map[k] for k in sorted(sent_map.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "INF =3  # query 1 vs its sentences\n",
    "\n",
    "q = extract_query_embedding(INF)\n",
    "sents = extract_sentence_embeddings(INF)\n",
    "\n",
    "# safety: check dimensions\n",
    "for k, s in enumerate(sents, start=1):\n",
    "    if s.shape != q.shape:\n",
    "        raise ValueError(f\"Dim mismatch at sentence {k}: {s.shape} vs {q.shape}\")\n",
    "\n",
    "# save CSV-like text: sentence_index,cosine_distance\n",
    "with open(cos_distance, \"w\", encoding=\"utf-8\") as out:\n",
    "    out.write(\"sentence_index,cosine_distance\\n\")\n",
    "    for idx, svec in enumerate(sents, start=1):\n",
    "        d = cosine_distance(q, svec)\n",
    "        out.write(f\"{idx},{d:.8f}\\n\")\n",
    "\n",
    "print(\"Saved:\", cos_distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Computing cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return 1.0 - float(a @ b / denom) if denom else 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "This below code calculates the cosine distance between query embeddings and sentence embeddings for inferences 1 to 10.\n",
    "\n",
    "For each inference, it writes the sentence index and corresponding cosine distance to a file, saving the results in a specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for inf in range (1, 11):\n",
    "    q = extract_query_embedding(inf)\n",
    "    sents = extract_sentence_embeddings(inf)\n",
    "\n",
    "    out_path = os.path.join(output_dir, f\"inference{inf}_cosine_distances.txt\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        out.write(\"sentence_index,cosine_distance\\n\")\n",
    "        for idx, svec in enumerate(sents, start=1):\n",
    "            d = float(cosine_distance(q, svec))   # SciPy cosine distance\n",
    "            out.write(f\"{idx},{d:.8f}\\n\")\n",
    "\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Now we will use gpt oss20b to give label to each sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Import Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup the async OpenAI client for Ollama\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Dummy key for Ollama\n",
    ")\n",
    "\n",
    "# File paths\n",
    "input_file = \"embeddings/output_chunks.txt\"\n",
    "output_file = \"embeddings/output_chunks_labeled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunks_file(file_path):\n",
    "    \"\"\"Parse the chunks file and extract sentences by inference\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by inference sections\n",
    "    sections = re.split(r'=== Inference (\\d+) ===', content)\n",
    "    \n",
    "    inferences = {}\n",
    "    for i in range(1, len(sections), 2):  # Skip empty first element, then take pairs\n",
    "        inference_num = int(sections[i])\n",
    "        inference_content = sections[i + 1].strip()\n",
    "        \n",
    "        # Extract query and sentences\n",
    "        lines = inference_content.split('\\n')\n",
    "        query = \"\"\n",
    "        sentences = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Query:\"):\n",
    "                query = line.replace(\"Query:\", \"\").strip()\n",
    "            elif line.startswith(\"Sentence\"):\n",
    "                # Extract sentence number and content\n",
    "                match = re.match(r'Sentence (\\d+): (.+)', line)\n",
    "                if match:\n",
    "                    sentence_num = int(match.group(1))\n",
    "                    sentence_text = match.group(2)\n",
    "                    sentences.append({\n",
    "                        'number': sentence_num,\n",
    "                        'text': sentence_text\n",
    "                    })\n",
    "        \n",
    "        inferences[inference_num] = {\n",
    "            'query': query,\n",
    "            'sentences': sentences\n",
    "        }\n",
    "    \n",
    "    return inferences\n",
    "\n",
    "# Test the parser\n",
    "data = parse_chunks_file(input_file)\n",
    "print(f\"Parsed {len(data)} inferences\")\n",
    "for inf_num, inf_data in data.items():\n",
    "    print(f\"Inference {inf_num}: Query='{inf_data['query']}', {len(inf_data['sentences'])} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Async Labeling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_simple_label(sentence_text):\n",
    "    try:\n",
    "        prompt = f\"\"\"You are an expert text analyzer. Your task is to read the following text and create a precise, descriptive label that captures its essence.\n",
    "\n",
    "TEXT: \"{sentence_text[:300]}\"\n",
    "\n",
    "HOW TO ANALYZE:\n",
    "1. First, identify the MAIN SUBJECT or topic being discussed\n",
    "2. Second, determine the FIELD or DOMAIN this belongs to\n",
    "3. Third, consider the SPECIFIC CONTEXT or approach being used\n",
    "4. Finally, create a label that combines the most important aspects\n",
    "\n",
    "THINKING PROCESS:\n",
    "- What is this text primarily about?\n",
    "- What field of knowledge does this belong to?\n",
    "- What specific aspect or angle is being discussed?\n",
    "- How would an expert in this field categorize this?\n",
    "\n",
    "LABEL REQUIREMENTS:\n",
    "- Use EXACTLY 2-3 words\n",
    "- Be specific and descriptive\n",
    "- Use clear, professional terminology\n",
    "- Capture the most important essence of the text\n",
    "\n",
    "EXAMPLES OF GOOD LABELS:\n",
    "- \"quantum mechanics\" (not just \"physics\")\n",
    "- \"data structures\" (not just \"programming\")  \n",
    "- \"financial modeling\" (not just \"business\")\n",
    "- \"cognitive psychology\" (not just \"psychology\")\n",
    "- \"organic synthesis\" (not just \"chemistry\")\n",
    "- \"machine learning\" (not just \"technology\")\n",
    "\n",
    "CREATE YOUR LABEL (2-3 words): \"\"\"\n",
    "        \n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-oss:20b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,  \n",
    "            max_tokens=500     \n",
    "        )\n",
    "        \n",
    "        label = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Clean up response - take only first 2-3 words\n",
    "        words = label.split()[:3]\n",
    "        return \" \".join(words) if words else \"general topic\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return \"classification error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "This cell, process all the input text, this is actually the main block which sends all the input text aync to the gpt oss\n",
    "and gpt oss label each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_sentences(data):\n",
    "    labeled_data = {}\n",
    "    \n",
    "    for inf_num, inf_data in data.items():\n",
    "        labeled_data[inf_num] = {\n",
    "            'query': inf_data['query'],\n",
    "            'sentences': []\n",
    "        }\n",
    "        \n",
    "        for sentence in inf_data['sentences']:\n",
    "            label = await generate_simple_label(sentence['text'])\n",
    "            \n",
    "            labeled_data[inf_num]['sentences'].append({\n",
    "                'number': sentence['number'],\n",
    "                'text': sentence['text'],\n",
    "                'label': label\n",
    "            })\n",
    "            \n",
    "            await asyncio.sleep(2)  # Simple delay\n",
    "    \n",
    "    return labeled_data\n",
    "\n",
    "labeled_data = await process_all_sentences(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "This block just save the labeled output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings/labeled_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for inf_num in sorted(labeled_data.keys()):\n",
    "        inf_data = labeled_data[inf_num]\n",
    "        \n",
    "        f.write(f\"=== Inference {inf_num} ===\\n\")\n",
    "        f.write(f\"Query: {inf_data['query']}\\n\")\n",
    "        \n",
    "        for sentence in sorted(inf_data['sentences'], key=lambda x: x['number']):\n",
    "            f.write(f\"Sentence {sentence['number']}: {sentence['text']}\\n\")\n",
    "            f.write(f\"Label: {sentence['label']}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The below cells create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-parse the updated labeled_output.txt file\n",
    "def parse_existing_labeled_data():\n",
    "    \"\"\"Parse the existing labeled_output.txt file\"\"\"\n",
    "    labeled_data = {}\n",
    "    \n",
    "    with open(\"embeddings/labeled_output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by inference sections\n",
    "    sections = content.split(\"=== Inference \")\n",
    "    \n",
    "    for section in sections[1:]:  # Skip first empty section\n",
    "        lines = section.strip().split('\\n')\n",
    "        inf_num = int(lines[0].split(' ===')[0])\n",
    "        \n",
    "        # Extract query\n",
    "        query_line = [line for line in lines if line.startswith(\"Query:\")][0]\n",
    "        query = query_line.replace(\"Query:\", \"\").strip()\n",
    "        \n",
    "        # Extract sentences and labels\n",
    "        sentences = []\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            if lines[i].startswith(\"Sentence \"):\n",
    "                # Extract sentence\n",
    "                sentence_text = lines[i].split(\": \", 1)[1] if \": \" in lines[i] else \"\"\n",
    "                sentence_num = int(lines[i].split(\":\")[0].split(\" \")[1])\n",
    "                \n",
    "                # Extract label (next line)\n",
    "                if i + 1 < len(lines) and lines[i + 1].startswith(\"Label:\"):\n",
    "                    label = lines[i + 1].replace(\"Label:\", \"\").strip()\n",
    "                else:\n",
    "                    label = \"Unknown\"\n",
    "                \n",
    "                sentences.append({\n",
    "                    'number': sentence_num,\n",
    "                    'text': sentence_text,\n",
    "                    'label': label\n",
    "                })\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        labeled_data[inf_num] = {\n",
    "            'query': query,\n",
    "            'sentences': sentences\n",
    "        }\n",
    "    \n",
    "    return labeled_data\n",
    "\n",
    "# RELOAD the labeled_data with updated labels\n",
    "labeled_data = parse_existing_labeled_data()\n",
    "print(f\"✅ Reloaded labeled_data with updated labels\")\n",
    "\n",
    "# Verify the new labels\n",
    "print(\"\\nNew labels:\")\n",
    "for inf_num in range (1,11):\n",
    "    print(f\"Inference {inf_num}:\")\n",
    "    for sentence in labeled_data[inf_num]['sentences']:\n",
    "        print(f\"  S{sentence['number']}: {sentence['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's extract the actual query texts for the first 10 inferences\n",
    "print(\"=== EXTRACTING QUERY TEXTS ===\")\n",
    "query_texts = {}\n",
    "for i in range(1, 11):\n",
    "    query_text = labeled_data[i]['query']\n",
    "    query_texts[i] = query_text\n",
    "    print(f\"Q{i}: '{query_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cosine distance ranges for first 10 inferences\n",
    "print(\"=== COSINE DISTANCE RANGES ===\")\n",
    "all_distances = []\n",
    "for i in range(1, 11):\n",
    "    distances_list = list(distances[i].values())\n",
    "    all_distances.extend(distances_list)\n",
    "    min_dist = min(distances_list)\n",
    "    max_dist = max(distances_list)\n",
    "    print(f\"Q{i} ({labeled_data[i]['query']}): {len(distances_list)} sentences, range: {min_dist:.4f} - {max_dist:.4f}\")\n",
    "\n",
    "overall_min = min(all_distances)\n",
    "overall_max = max(all_distances)\n",
    "print(f\"\\n=== OVERALL RANGE ===\")\n",
    "print(f\"Min distance: {overall_min:.4f}\")\n",
    "print(f\"Max distance: {overall_max:.4f}\")\n",
    "print(f\"Suggested x-axis range: {overall_min-0.05:.2f} to {overall_max+0.05:.2f}\")\n",
    "\n",
    "# Also check sentence order for one example\n",
    "print(f\"\\n=== SENTENCE ORDER CHECK (Q2 Example) ===\")\n",
    "q2_sentences = sorted(labeled_data[2]['sentences'], key=lambda x: x['number'])\n",
    "q2_distances = distances[2]\n",
    "print(\"Sentence order (left-to-right in graph):\")\n",
    "for sent in q2_sentences:\n",
    "    sent_num = sent['number']\n",
    "    if sent_num in q2_distances:\n",
    "        dist = q2_distances[sent_num]\n",
    "        label = sent['label'][:30] + \"...\" if len(sent['label']) > 30 else sent['label']\n",
    "        print(f\"  S{sent_num}: distance={dist:.4f}, label='{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_graph_with_distribution():\n",
    "    \"\"\"Create graph with distribution summary using actual query data\"\"\"\n",
    "    \n",
    "    # Create figure with two panels\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    gs = fig.add_gridspec(1, 3, width_ratios=[3, 1, 0.1])  # Main plot, box plot, spacer\n",
    "    \n",
    "    # Extract actual queries from labeled_data (first 10)\n",
    "    queries = {}\n",
    "    for i in range(1, 11):\n",
    "        queries[i] = labeled_data[i]['query']\n",
    "    \n",
    "    print(\"Using actual queries:\", queries)\n",
    "    \n",
    "    # Generate colors for 10 queries\n",
    "    colors_list = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#4CAF50', \n",
    "                   '#9C27B0', '#FF9800', '#607D8B', '#E91E63', '#795548']\n",
    "    colors = {queries[i]: colors_list[i-1] for i in range(1, 11)}\n",
    "    \n",
    "    # Panel A: Main scatter plot\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    for inf_num in range(1, 11):\n",
    "        query = queries[inf_num]\n",
    "        \n",
    "        # Sort sentences by number for left-to-right order\n",
    "        for sent_num in sorted(distances[inf_num].keys()):\n",
    "            distance = distances[inf_num][sent_num]\n",
    "            ax1.scatter(distance, inf_num, c=colors[query], s=200, alpha=0.8, \n",
    "                       edgecolors='black', linewidth=1, zorder=3)\n",
    "    \n",
    "    ax1.set_ylim(0.5, 10.5)\n",
    "    ax1.set_yticks(range(1, 11))\n",
    "    ax1.set_yticklabels(list(queries.values()), fontsize=14)\n",
    "    ax1.set_xlim(0.58, 0.99)\n",
    "    ax1.set_xlabel('Cosine Distance', fontweight='bold', fontsize=18)\n",
    "    ax1.set_ylabel('Query Type', fontweight='bold', fontsize=18)\n",
    "    ax1.grid(True, alpha=0.3, zorder=1)\n",
    "    ax1.set_title('A. Semantic Distance Distribution', fontweight='bold', fontsize=18, loc='left')\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax1.set_facecolor('#fafafa')\n",
    "    \n",
    "    # Panel B: Distribution Summary (Box Plot)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    box_colors = []\n",
    "    \n",
    "    for inf_num in range(1, 11):\n",
    "        query = queries[inf_num]\n",
    "        dists = list(distances[inf_num].values())\n",
    "        box_data.append(dists)\n",
    "        box_labels.append(f\"{query}\\n(n={len(dists)})\")\n",
    "        box_colors.append(colors[query])\n",
    "    \n",
    "    bp = ax2.boxplot(box_data, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for patch, color in zip(bp['boxes'], box_colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_xticklabels(box_labels, rotation=45, ha='right', fontsize=10)\n",
    "    ax2.set_ylabel('Cosine Distance', fontweight='bold', fontsize=16)\n",
    "    ax2.set_title('B. Distribution\\nSummary', fontweight='bold', fontsize=16, loc='left')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle('Semantic Distance Analysis', \n",
    "                 fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Run the graph with actual queries\n",
    "create_graph_with_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_readable_table():\n",
    "    \"\"\"Create a clean, readable table with proper spacing and fonts\"\"\"\n",
    "    \n",
    "    # Extract actual queries from labeled_data (first 10)\n",
    "    queries = {}\n",
    "    for i in range(1, 11):\n",
    "        queries[i] = labeled_data[i]['query']\n",
    "    \n",
    "    # Same colors\n",
    "    colors_list = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#4CAF50', \n",
    "                   '#9C27B0', '#FF9800', '#607D8B', '#E91E63', '#795548']\n",
    "    colors = {queries[i]: colors_list[i-1] for i in range(1, 11)}\n",
    "    \n",
    "    def wrap_label(label, max_chars_per_line=20):\n",
    "        \"\"\"Wrap long labels to fit better\"\"\"\n",
    "        if len(label) <= max_chars_per_line:\n",
    "            return label\n",
    "        \n",
    "        words = label.split()\n",
    "        lines = []\n",
    "        current_line = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for word in words:\n",
    "            if current_length + len(word) + 1 <= max_chars_per_line:\n",
    "                current_line.append(word)\n",
    "                current_length += len(word) + 1\n",
    "            else:\n",
    "                if current_line:\n",
    "                    lines.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "                current_length = len(word)\n",
    "        \n",
    "        if current_line:\n",
    "            lines.append(' '.join(current_line))\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    # Build table with actual data\n",
    "    max_sentences = max(len(labeled_data[i]['sentences']) for i in range(1, 11))\n",
    "    headers = ['Query'] + [f'Sentence {i+1}' for i in range(max_sentences)]\n",
    "    table_data = [headers]\n",
    "\n",
    "    for inf_num in range(1, 11):\n",
    "        query = queries[inf_num]\n",
    "        row = [query]\n",
    "        \n",
    "        inf_sentences = labeled_data[inf_num]['sentences']\n",
    "        sorted_sentences = sorted(inf_sentences, key=lambda x: x['number'])\n",
    "        \n",
    "        for i in range(max_sentences):\n",
    "            if i < len(sorted_sentences):\n",
    "                label = wrap_label(sorted_sentences[i]['label'])\n",
    "                row.append(label)\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "        \n",
    "        table_data.append(row)\n",
    "    \n",
    "    # Create optimized table\n",
    "    fig, ax = plt.subplots(figsize=(max_sentences * 3, 12))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table with proper spacing\n",
    "    table = ax.table(cellText=table_data, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)  # Readable but not too big\n",
    "    table.scale(1, 3)  # Good height without too much empty space\n",
    "    \n",
    "    # Style header row\n",
    "    for i in range(len(headers)):\n",
    "        table[(0, i)].set_facecolor('#34495e')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white', fontsize=14)\n",
    "        table[(0, i)].set_height(0.08)\n",
    "    \n",
    "    # Style data rows\n",
    "    for i in range(1, len(table_data)):\n",
    "        query = table_data[i][0]\n",
    "        for j in range(len(headers)):\n",
    "            if j == 0:  # Query column\n",
    "                table[(i, j)].set_facecolor(colors[query])\n",
    "                table[(i, j)].set_alpha(0.9)\n",
    "                table[(i, j)].set_text_props(weight='bold', color='white', fontsize=13)\n",
    "            else:  # Sentence columns\n",
    "                if table_data[i][j]:\n",
    "                    table[(i, j)].set_facecolor(colors[query])\n",
    "                    table[(i, j)].set_alpha(0.2)\n",
    "                    table[(i, j)].set_text_props(fontsize=11)\n",
    "                else:\n",
    "                    table[(i, j)].set_facecolor('#f8f9fa')\n",
    "            \n",
    "            table[(i, j)].set_height(0.06)  # Compact but readable\n",
    "    \n",
    "    plt.title('Sentence Labels: Left-to-Right Order Matches Graph Dots Above', \n",
    "              fontweight='bold', fontsize=18, pad=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the readable table\n",
    "create_readable_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
